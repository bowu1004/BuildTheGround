![build_the_ground](https://img.shields.io/badge/self--improvement-miscellaneous-brightgreen)

# Build a solid ground
###### Wu, B.
---

## ☆ Camera Calibration using Python

## ☆ Git

## ☆ STFT and Wavelet

## ☆ Kalman filter

## ☆ Relations between Trace, Gradient, Divergence, Curl, Laplacian, Jacobian and Hessian.

## ☆ Loss function of CrossEntropy, KL divergence(RelativeEntropy), JS divergence, and Wasserstein distance.

## ☆ TBD-publish python packages at https://packaging.python.org/tutorials/packaging-projects/

## ☆ ML note 1: Linear Discriminant Function 

## ☆ Math: eigenvalue, eigenvector, rank/trace

## ☆ ML note 2: Logistic Regression

## ☆ ML note 3: Maximum Likelihood Estimaiton, vs., Maximum A Posteriori estimation

## ☆ ML note 4: Implement A Softmax Classifier and A Simple MLP NN.

## ☆ MFCC, Mel/Gammatone Spectrogram

## ☆ Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization

## ☆ L1 vs L2 regularization
The main intuitive difference between the L1 and L2 regularization is that L1 regularization tries to estimate the median of the data while the L2 regularization tries to estimate the mean of the data to avoid overfitting.

The key difference between these techniques is that Lasso shrinks the less important feature’s coefficient to zero thus, removing some features altogether. So, this works well for feature selection in case we have a huge number of features.

## ☆ Why normal distribution is common?
See Central Limit Theorem

## ☆ Understand boxplot
See https://zhuanlan.zhihu.com/p/347067055

## ☆ TBD

---

### This repository starts at
+ [x] July-07-2020
+ [x] July-20-2020
+ [x] Jan-12-2021
+ [x] Feb-16-2021
+ [x] Feb-24-2021
+ [x] Feb-26-2021
+ [x] March-10-2021
+ [x] March-21-2021
+ [x] April-03-2021
+ [x] Sept-07-2021

and, in progress...
